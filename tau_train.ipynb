{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec2530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TAU 모델 학습 스크립트\n",
    "기존 URP_baseline.ipynb 기반 - 바로 실행 가능\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "import netCDF4 as nc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import *\n",
    "from tau_frost_model import get_tau_model\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5462b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "718fe994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output path name: results/asos_16ch_time[-18, -15, -12]_2km_TAU_early_fusion\n",
      "ASOS weight: 1.00, AAFOS weight: 0.00\n"
     ]
    }
   ],
   "source": [
    "# ====== 실험 설정 ====== #\n",
    "seeds = [0]  # 10번 실험\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # 입력 크기가 고정되면 속도 향상\n",
    "torch.backends.cudnn.deterministic = False  # 재현성보다 속도 우선\n",
    "\n",
    "# 데이터 설정\n",
    "ASOS = True\n",
    "AAFOS = False\n",
    "assert ASOS or AAFOS, \"At least one of ASOS or AAFOS must be True.\"\n",
    "\n",
    "channels = '16ch'\n",
    "time_range = [-18, -15, -12]  # TAU: 4개 시점 (baseline은 [-12])\n",
    "n_times = len(time_range)\n",
    "resolution = '2km'\n",
    "postfix = 'TAU_early_fusion'\n",
    "tau_n_heads = 8\n",
    "tau_n_layers = 2\n",
    "\n",
    "# 출력 경로\n",
    "output_path = \"results/\"\n",
    "output_path += 'asos_' if ASOS else ''\n",
    "output_path += 'aafos_' if AAFOS else ''\n",
    "output_path += channels + '_'\n",
    "output_path += 'time' + str(time_range) + '_'\n",
    "output_path += resolution\n",
    "output_path += ('_' + postfix) if postfix != '' else postfix\n",
    "print(f\"Output path name: {output_path}\")\n",
    "\n",
    "# ASOS:AAFOS 비율\n",
    "asos_aafos_ratio = 5.0\n",
    "asos_weight = asos_aafos_ratio / (asos_aafos_ratio + 1.0 * AAFOS) if ASOS else 0.0\n",
    "aafos_weight = 1.0 / (asos_aafos_ratio * ASOS + 1.0) if AAFOS else 0.0\n",
    "print(f\"ASOS weight: {asos_weight:.2f}, AAFOS weight: {aafos_weight:.2f}\")\n",
    "\n",
    "# 기타 설정\n",
    "latlon = False\n",
    "central_patch = False\n",
    "use_patch = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04e0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 채널 설정 ====== #\n",
    "if channels == '16ch':\n",
    "    channels_name = ['vi004','vi005','vi006','vi008','nr013','nr016','sw038',\n",
    "                     'wv063','wv069','wv073','ir087','ir096','ir105','ir112','ir123','ir133']\n",
    "    channels_calib = ['vi004','vi005','vi006','vi008','nr013','nr016','sw038',\n",
    "                      'wv063','wv069','wv073','ir087','ir096','ir105','ir112','ir123','ir133']\n",
    "    \n",
    "    channels_mean = [1.1912e-01, 1.1464e-01, 1.0734e-01, 1.2504e-01, 5.4983e-02, 9.0381e-02,\n",
    "                     2.7813e+02, 2.3720e+02, 2.4464e+02, 2.5130e+02, 2.6948e+02, 2.4890e+02,\n",
    "                     2.7121e+02, 2.7071e+02, 2.6886e+02, 2.5737e+02]\n",
    "    channels_std = [0.1306, 0.1303, 0.1306, 0.1501, 0.0268, 0.0838, 15.8211, 6.1468,\n",
    "                    7.8054, 9.3251, 16.4265, 9.6150, 17.2518, 17.6064, 17.0090, 12.5026]\n",
    "else:\n",
    "    raise ValueError(\"Invalid channels.\")\n",
    "\n",
    "n_channels = len(channels_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f26657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total channels: 51\n",
      "ASOS stations: [93, 108, 112, 119, 131, 133, 136, 143, 146, 156, 177, 102, 104, 115, 138, 152, 155, 159, 165, 168, 169, 184, 189]\n"
     ]
    }
   ],
   "source": [
    "# ====== 데이터 설정 ====== #\n",
    "train_data_info_list = []\n",
    "if ASOS:\n",
    "    train_data_info_list.append({\n",
    "        'label_type': 'asos',\n",
    "        'start_date_str': '20200101',\n",
    "        'end_date_str': '20230630',\n",
    "        'hour_col_pairs': [(6, 'AM')],\n",
    "        'label_keys': ['93','108','112','119','131','133','136','143','146','156','177',\n",
    "                       '102','104','115','138','152','155','159','165','168','169','184','189']\n",
    "    })\n",
    "\n",
    "test_asos_data_info_list = [\n",
    "    {\n",
    "        'label_type': 'asos',\n",
    "        'start_date_str': '20230701',\n",
    "        'end_date_str': '20240630',\n",
    "        'hour_col_pairs': [(6, 'AM')],\n",
    "        'label_keys': ['93','108','112','119','131','133','136','143','146','156','177',\n",
    "                       '102','104','115','138','152','155','159','165','168','169','184','189']\n",
    "    },\n",
    "] if ASOS else None\n",
    "\n",
    "# 이미지 크기 설정\n",
    "origin_size = 900 if resolution == '2km' else 1800\n",
    "image_size = 512 if resolution == '2km' else 1024\n",
    "patch_size = 512\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = '/home/dm4/repo/data/kma_data/date_kst_URP'\n",
    "\n",
    "misc_channels = {\n",
    "    'elevation': 'elevation_1km_3600.npy',\n",
    "    'vegetation': 'vegetation_1km_3600.npy',\n",
    "    'watermap': 'watermap_1km_avg_3600.npy'\n",
    "}\n",
    "lat_lon_path = 'assets/gk2a_ami_ko010lc_latlon.nc'\n",
    "\n",
    "# 학습 설정\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "decay = [10, 20]\n",
    "lr_decay = 0.1\n",
    "weight_decay = 1e-5\n",
    "threshold = [0.25]\n",
    "\n",
    "# TAU 모델 설정\n",
    "fusion_type = 'early'  # 'early' or 'late'\n",
    "tau_n_heads = 8\n",
    "tau_n_layers = 2\n",
    "\n",
    "# ====== 자동 계산 값 ====== #\n",
    "asos_x_base, asos_y_base, asos_image_size = get_crop_base(image_size, label_type='asos')\n",
    "aafos_x_base, aafos_y_base, aafos_image_size = get_crop_base(image_size, label_type='aafos')\n",
    "aafos_x_base -= asos_x_base\n",
    "aafos_y_base -= asos_y_base\n",
    "\n",
    "total_channels = len(channels_name) * len(time_range) + len(misc_channels.keys())\n",
    "total_channels += 2 if latlon else 0\n",
    "print(f\"Total channels: {total_channels}\")\n",
    "\n",
    "# ====== 관측소 좌표 설정 ====== #\n",
    "asos_land_map = {k: coord_to_map(*v, origin_size) for k, v in ASOS_LAND_COORD.items()}\n",
    "asos_land_map = {k: (v[0] - asos_x_base, v[1] - asos_y_base) for k, v in asos_land_map.items()}\n",
    "asos_coast_map = {k: coord_to_map(*v, origin_size) for k, v in ASOS_COAST_COORD.items()}\n",
    "asos_coast_map = {k: (v[0] - asos_x_base, v[1] - asos_y_base) for k, v in asos_coast_map.items()}\n",
    "asos_map_dict = {**asos_land_map, **asos_coast_map}\n",
    "print(f\"ASOS stations: {list(asos_map_dict.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d771fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISC images shape: torch.Size([3, 512, 512])\n",
      "Total mean length: 51\n",
      "Total std length: 51\n"
     ]
    }
   ],
   "source": [
    "# ====== Mean/Std 설정 ====== #\n",
    "channels_mean = channels_mean * len(time_range)\n",
    "channels_std = channels_std * len(time_range)\n",
    "\n",
    "# ====== Misc Images 로드 ====== #\n",
    "misc_images = []\n",
    "for misc_channel, misc_path in misc_channels.items():\n",
    "    misc_image = np.load(f'assets/misc_channels/{misc_path}', allow_pickle=True)\n",
    "    misc_image = cv2.resize(misc_image, (asos_image_size, asos_image_size), interpolation=cv2.INTER_CUBIC)\n",
    "    misc_images.append(misc_image)\n",
    "misc_images = np.stack(misc_images, axis=0)\n",
    "misc_images = torch.tensor(misc_images, dtype=torch.float32)\n",
    "\n",
    "if latlon:\n",
    "    lat_lon_data = nc.Dataset(lat_lon_path)\n",
    "    lat = lat_lon_data['lat'][:].data\n",
    "    lon = lat_lon_data['lon'][:].data\n",
    "    lat = cv2.resize(lat, (origin_size, origin_size), interpolation=cv2.INTER_CUBIC)\n",
    "    lon = cv2.resize(lon, (origin_size, origin_size), interpolation=cv2.INTER_CUBIC)\n",
    "    asos_lat = lat[asos_y_base:asos_y_base + asos_image_size, asos_x_base:asos_x_base + asos_image_size]\n",
    "    asos_lon = lon[asos_y_base:asos_y_base + asos_image_size, asos_x_base:asos_x_base + asos_image_size]\n",
    "    \n",
    "    lat_image = cv2.resize(asos_lat, (asos_image_size, asos_image_size), interpolation=cv2.INTER_CUBIC)\n",
    "    lon_image = cv2.resize(asos_lon, (asos_image_size, asos_image_size), interpolation=cv2.INTER_CUBIC)\n",
    "    lat_image = torch.tensor(lat_image, dtype=torch.float32).unsqueeze(0)\n",
    "    lon_image = torch.tensor(lon_image, dtype=torch.float32).unsqueeze(0)\n",
    "    misc_images = torch.cat([misc_images, lat_image, lon_image], dim=0)\n",
    "\n",
    "print(f'MISC images shape: {misc_images.shape}')\n",
    "\n",
    "# ====== n_misc 계산 ====== #\n",
    "n_misc = len(misc_channels) + (2 if latlon else 0)\n",
    "\n",
    "# Normalize misc\n",
    "misc_mean = misc_images.mean(dim=[1, 2], keepdim=True)\n",
    "misc_std = misc_images.std(dim=[1, 2], keepdim=True)\n",
    "\n",
    "total_mean = channels_mean + misc_mean.squeeze().tolist()\n",
    "total_std = channels_std + misc_std.squeeze().tolist()\n",
    "\n",
    "print(f'Total mean length: {len(total_mean)}')\n",
    "print(f'Total std length: {len(total_std)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037b46d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASOS patch candidates shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# ====== Patch Candidates 생성 ====== #\n",
    "asos_patch_candidate = np.zeros([image_size, image_size], dtype=np.uint8)\n",
    "for x, y in asos_map_dict.values():\n",
    "    y_min = np.clip(y - 3 * patch_size // 4, 0, image_size - patch_size + 1)\n",
    "    y_max = np.clip(y - patch_size // 4, 0, image_size - patch_size + 1)\n",
    "    x_min = np.clip(x - 3 * patch_size // 4, 0, image_size - patch_size + 1)\n",
    "    x_max = np.clip(x - patch_size // 4, 0, image_size - patch_size + 1)\n",
    "    asos_patch_candidate[y_min:y_max, x_min:x_max] = 1\n",
    "\n",
    "asos_patch_candidate = np.argwhere(asos_patch_candidate == 1)[:, [1, 0]]\n",
    "print(f'ASOS patch candidates shape: {asos_patch_candidate.shape}')\n",
    "\n",
    "patch_candidates = {'asos': asos_patch_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6ba4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Transform 설정 ====== #\n",
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=total_mean, std=total_std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "884110c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_measure_valid(Y_test, Y_test_hat, cutoff=0.5):\n",
    "    Y_test = Y_test.ravel()\n",
    "    Y_test_hat = Y_test_hat.ravel()\n",
    "    \n",
    "    Y_valid = (~np.isnan(Y_test))\n",
    "    Y_test = Y_test[Y_valid]\n",
    "    Y_test_hat = Y_test_hat[Y_valid]\n",
    "\n",
    "    cfmat = confusion_matrix(Y_test, Y_test_hat > cutoff, labels = [0,1])\n",
    "    acc = np.trace(cfmat) / np.sum(cfmat)\n",
    "    csi = cfmat[1,1] /(np.sum(cfmat) - cfmat[0,0] + 1e-8)\n",
    "    \n",
    "    try:\n",
    "        auroc = roc_auc_score(Y_test, Y_test_hat)\n",
    "    except Exception as e:\n",
    "        auroc = 0.0\n",
    "\n",
    "    return csi, acc, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d41aed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== TAU용 train 함수 (baseline 손실함수 유지) ====== #\n",
    "def train(model, images, labels, coords, map_dict, cls_num=0):\n",
    "    \"\"\"\n",
    "    TAU 모델용 학습 함수 - baseline과 동일한 손실함수 사용\n",
    "    \n",
    "    Args:\n",
    "        model: TAUDeepLab 모델\n",
    "        images: (B, T*C + misc, H, W) 입력 이미지 (이미 cuda로 이동됨)\n",
    "        labels: (B, N_stations) 관측소별 라벨\n",
    "        coords: (B, 2) 패치 좌표 [px, py]\n",
    "        map_dict: {station_id: (x, y)} 관측소 좌표\n",
    "        cls_num: 클래스 인덱스 (ASOS=0, AAFOS=1)\n",
    "    \n",
    "    Returns:\n",
    "        loss: Focal Loss + Non-frost Loss + CSI Loss\n",
    "    \"\"\"\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "    \n",
    "    # TAU 모델 forward - 입력 형태: (B, T*C + misc, H, W)\n",
    "    pred_map = model.forward(images)[:, cls_num]  # (B, H, W)\n",
    "    \n",
    "    # 관측소 위치에서 prediction 추출 (패치 좌표 고려)\n",
    "    pred_vec = torch.zeros_like(labels)\n",
    "    for b, (px, py) in enumerate(coords):\n",
    "        px, py = int(px), int(py)\n",
    "        for i, (x, y) in enumerate(map_dict.values()):\n",
    "            # 패치 내부에 있는 관측소만 prediction 사용\n",
    "            if px <= x < px + patch_size and py <= y < py + patch_size:\n",
    "                pred_vec[b, i] = pred_map[b, y - py, x - px]\n",
    "            else:\n",
    "                # 패치 외부 관측소는 라벨 값으로 대체 (loss 기여 없음)\n",
    "                pred_vec[b, i] = labels[b, i]\n",
    "    \n",
    "    # Valid mask (NaN 제외)\n",
    "    labels_valid = (~torch.isnan(labels)).float()\n",
    "    labels = torch.nan_to_num(labels, 0.0)\n",
    "    \n",
    "    # 1. Focal Loss\n",
    "    loss_focal_raw = sigmoid_focal_loss(pred_vec, labels, alpha=-1, gamma=2, reduction='none')\n",
    "    loss_focal = (loss_focal_raw * labels_valid).sum() / labels_valid.sum().clamp(min=1.0)\n",
    "    \n",
    "    # 2. Non-frost Loss (배치 내 모든 라벨이 0일 때 전체 맵에 대해 BCE 적용)\n",
    "    valid_any_batch = (labels_valid.sum() > 0)\n",
    "    all_zero_batch = ((labels * labels_valid).sum() == 0)\n",
    "    \n",
    "    if not (valid_any_batch and all_zero_batch):\n",
    "        loss_non_frost = torch.tensor(0.0, device=labels.device)\n",
    "    else:\n",
    "        loss_non_frost = binary_cross_entropy_with_logits(\n",
    "            pred_map, torch.zeros_like(pred_map), reduction='mean'\n",
    "        )\n",
    "    \n",
    "    # 3. CSI Loss (미분 가능한 CSI 근사)\n",
    "    pred_prob = torch.sigmoid(pred_vec)\n",
    "    tp = torch.sum(pred_prob * labels * labels_valid, dim=0)\n",
    "    fn = torch.sum((1 - pred_prob) * labels * labels_valid, dim=0)\n",
    "    fp = torch.sum(pred_prob * (1 - labels) * labels_valid, dim=0)\n",
    "    \n",
    "    # CSI = TP / (TP + FN + FP) → -log(CSI) 최소화\n",
    "    loss_csi = torch.mean(-torch.log(tp + 1e-10) + torch.log(tp + fn + fp + 1e-10))\n",
    "    \n",
    "    return loss_focal + loss_non_frost + loss_csi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e54efded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TAU Frost Forecasting Model Training\n",
      "============================================================\n",
      "Fusion type: early\n",
      "Time range: [-18, -15, -12]\n",
      "N times: 3, N channels: 16\n",
      "============================================================\n",
      "\n",
      "Creating datasets...\n",
      "== Preparing asos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing asos:   1%|▍                                          | 12/1277 [00:00<00:11, 113.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 20200101 AM skipped, 2019-12-31 12:00:00 not in date_table\n",
      "  - 20200104 AM skipped, 2020-01-03 12:00:00 not in date_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing asos:  51%|█████████████████████▎                    | 648/1277 [00:03<00:01, 399.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - 20211003 AM skipped, 2021-10-02 12:00:00 not in date_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing asos: 100%|█████████████████████████████████████████| 1277/1277 [00:06<00:00, 182.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  - Total 1274 image-label pairs prepared\n",
      "  - train_asos_image_label_list.yaml saved\n",
      "\n",
      "== asos dataset length synced to 1274\n",
      "GK2A Dataset initialized\n",
      "\n",
      "\n",
      "== Preparing asos...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing asos: 100%|███████████████████████████████████████████| 366/366 [00:02<00:00, 182.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  - Total 366 image-label pairs prepared\n",
      "  - test_asos_image_label_list.yaml saved\n",
      "\n",
      "== asos dataset length synced to 366\n",
      "GK2A Dataset initialized\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "Seed 0\n",
      "============================================================\n",
      "모델 파라미터 수: 12,305,490\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  0: 100%|██████████| 637/637 [20:55<00:00,  1.97s/it, loss=0.9736, asos=0.9736]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 - Total: 7.2798, ASOS: 7.2798, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.0549, AUROC 0.5285 *\n",
      "\t   - Land:  CSI 0.0700, AUROC 0.5208\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5000\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 637/637 [20:59<00:00,  1.98s/it, loss=4.5024, asos=4.5024]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 - Total: 6.6243, ASOS: 6.6243, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.0202, AUROC 0.7021 \n",
      "\t   - Land:  CSI 0.0253, AUROC 0.6591\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5685\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  2: 100%|██████████| 637/637 [20:15<00:00,  1.91s/it, loss=2.2354, asos=2.2354]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 - Total: 6.0050, ASOS: 6.0050, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2764, AUROC 0.7795 *\n",
      "\t   - Land:  CSI 0.3500, AUROC 0.8332\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5609\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  3: 100%|██████████| 637/637 [23:45<00:00,  2.24s/it, loss=0.0070, asos=0.0070]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 - Total: 5.1003, ASOS: 5.1003, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.4071, AUROC 0.7947 *\n",
      "\t   - Land:  CSI 0.4964, AUROC 0.8556\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5389\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  4: 100%|██████████| 637/637 [23:28<00:00,  2.21s/it, loss=5.7848, asos=5.7848]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 - Total: 4.7282, ASOS: 4.7282, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.3867, AUROC 0.8346 \n",
      "\t   - Land:  CSI 0.4778, AUROC 0.9179\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5064\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  5: 100%|██████████| 637/637 [23:28<00:00,  2.21s/it, loss=0.0535, asos=0.0535]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 - Total: 4.4513, ASOS: 4.4513, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.1742, AUROC 0.6862 \n",
      "\t   - Land:  CSI 0.1867, AUROC 0.5662\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5000\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  6: 100%|██████████| 637/637 [23:23<00:00,  2.20s/it, loss=5.0439, asos=5.0439]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 - Total: 4.6727, ASOS: 4.6727, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2750, AUROC 0.8198 \n",
      "\t   - Land:  CSI 0.3404, AUROC 0.8765\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5500\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  7: 100%|██████████| 637/637 [23:20<00:00,  2.20s/it, loss=0.4335, asos=0.4335]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 - Total: 4.7558, ASOS: 4.7558, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.3055, AUROC 0.8254 \n",
      "\t   - Land:  CSI 0.3793, AUROC 0.8866\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5477\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  8: 100%|██████████| 637/637 [23:19<00:00,  2.20s/it, loss=10.3959, asos=10.3959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 - Total: 4.5642, ASOS: 4.5642, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2746, AUROC 0.7600 \n",
      "\t   - Land:  CSI 0.3405, AUROC 0.7831\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5133\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  9: 100%|██████████| 637/637 [23:22<00:00,  2.20s/it, loss=0.2829, asos=0.2829]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 - Total: 4.6409, ASOS: 4.6409, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2502, AUROC 0.7930 \n",
      "\t   - Land:  CSI 0.3160, AUROC 0.8405\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5331\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 637/637 [23:20<00:00,  2.20s/it, loss=0.0005, asos=0.0005]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Total: 4.3123, ASOS: 4.3123, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2919, AUROC 0.7882 \n",
      "\t   - Land:  CSI 0.3654, AUROC 0.8374\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5324\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 637/637 [23:25<00:00,  2.21s/it, loss=0.2024, asos=0.2024]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Total: 4.1959, ASOS: 4.1959, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.3460, AUROC 0.8044 \n",
      "\t   - Land:  CSI 0.4283, AUROC 0.8497\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5433\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 637/637 [23:29<00:00,  2.21s/it, loss=0.1642, asos=0.1642]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Total: 4.2040, ASOS: 4.2040, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2371, AUROC 0.7859 \n",
      "\t   - Land:  CSI 0.2987, AUROC 0.8277\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5290\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 637/637 [23:36<00:00,  2.22s/it, loss=0.0005, asos=0.0005]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Total: 4.0846, ASOS: 4.0846, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.1490, AUROC 0.7946 \n",
      "\t   - Land:  CSI 0.1901, AUROC 0.8415\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5312\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 637/637 [23:40<00:00,  2.23s/it, loss=5.0095, asos=5.0095]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Total: 4.0948, ASOS: 4.0948, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2341, AUROC 0.8020 \n",
      "\t   - Land:  CSI 0.2965, AUROC 0.8548\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5331\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 637/637 [23:30<00:00,  2.21s/it, loss=7.5213, asos=7.5213]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Total: 4.0364, ASOS: 4.0364, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.1833, AUROC 0.7926 \n",
      "\t   - Land:  CSI 0.2323, AUROC 0.8341\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5356\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 637/637 [23:33<00:00,  2.22s/it, loss=2.0528, asos=2.0528]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Total: 4.0561, ASOS: 4.0561, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.1738, AUROC 0.8105 \n",
      "\t   - Land:  CSI 0.2219, AUROC 0.8763\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5356\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 637/637 [23:24<00:00,  2.20s/it, loss=4.2481, asos=4.2481]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Total: 4.0476, ASOS: 4.0476, AAFOS: 0.0000\n",
      "\t - ASOS (T=0.25): CSI 0.2819, AUROC 0.7976 \n",
      "\t   - Land:  CSI 0.3561, AUROC 0.8581\n",
      "\t   - Coast: CSI 0.0000, AUROC 0.5335\n",
      "\n",
      "== asos dataset length synced to 1274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18:  51%|█████▏    | 328/637 [12:42<11:58,  2.33s/it, loss=12.8180, asos=12.8180]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 168\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;66;03m# Scaler를 사용한 backward 및 optimizer step\u001b[39;00m\n\u001b[32m    167\u001b[39m scaler.scale(loss).backward()\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m scaler.update()\n\u001b[32m    171\u001b[39m total_loss_asos += loss_asos.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dm\\anaconda3\\envs\\ST_env\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:452\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    446\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    449\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    450\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dm\\anaconda3\\envs\\ST_env\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:349\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    343\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    348\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    350\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\dm\\anaconda3\\envs\\ST_env\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:349\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\n\u001b[32m    342\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    343\u001b[39m     optimizer: torch.optim.Optimizer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    346\u001b[39m     **kwargs: Any,\n\u001b[32m    347\u001b[39m ) -> Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    348\u001b[39m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    350\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ====== 메인 학습 루프 ====== #\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TAU Frost Forecasting Model Training\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Fusion type: {fusion_type}\")\n",
    "    print(f\"Time range: {time_range}\")\n",
    "    print(f\"N times: {n_times}, N channels: {n_channels}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ====== 데이터셋 생성 ====== #\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = GK2ADataset(\n",
    "        data_path=data_path,\n",
    "        output_path=output_path,\n",
    "        data_info_list=train_data_info_list,\n",
    "        channels=channels,\n",
    "        time_range=time_range,\n",
    "        channels_calib=channels_calib,\n",
    "        image_size=image_size,\n",
    "        misc_images=misc_images,\n",
    "        patch_size=patch_size,\n",
    "        patch_candidates=patch_candidates,\n",
    "        transform=transform,\n",
    "        train=True\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        drop_last=True,\n",
    "        pin_memory=True  # GPU 전송 속도 향상\n",
    "    )\n",
    "    \n",
    "    if ASOS:\n",
    "        test_asos_dataset = GK2ADataset(\n",
    "            data_path=data_path,\n",
    "            output_path=output_path,\n",
    "            data_info_list=test_asos_data_info_list,\n",
    "            channels=channels,\n",
    "            time_range=time_range,\n",
    "            channels_calib=channels_calib,\n",
    "            image_size=image_size,\n",
    "            misc_images=misc_images,\n",
    "            patch_size=patch_size,\n",
    "            patch_candidates=None,\n",
    "            transform=transform,\n",
    "            train=False\n",
    "        )\n",
    "        test_asos_dataloader = DataLoader(\n",
    "            test_asos_dataset,\n",
    "            batch_size=batch_size // 2,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    # ====== Seed 별 학습 ====== #\n",
    "    for seed in seeds:\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Seed {seed}')\n",
    "        print(f'{\"=\"*60}')\n",
    "        \n",
    "        if os.path.exists(f'{output_path}/{seed}/ckpt.pt'):\n",
    "            print(f'Seed {seed} already done. Skipping...')\n",
    "            continue\n",
    "        \n",
    "        # 시드 설정\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # TAU 모델 생성\n",
    "        model = get_tau_model(\n",
    "            total_channels=total_channels,\n",
    "            patch_size=patch_size,\n",
    "            n_channels=n_channels,\n",
    "            n_misc=n_misc,\n",
    "            n_times=n_times,\n",
    "            num_classes=2,\n",
    "            tau_n_heads=tau_n_heads,\n",
    "            tau_n_layers=tau_n_layers\n",
    "        )\n",
    "        model = model.cuda()\n",
    "\n",
    "        print(f\"모델 파라미터 수: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decay, gamma=lr_decay)\n",
    "        \n",
    "        # AMP GradScaler 초기화\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        start_epoch = 0\n",
    "        \n",
    "        # 결과 저장\n",
    "        results = dict(\n",
    "            loss={'asos': [], 'aafos': [], 'total': []},\n",
    "            csi={'asos': {}, 'aafos': {}},\n",
    "            acc={'asos': {}, 'aafos': {}},\n",
    "            auroc={'asos': [], 'aafos': []},\n",
    "            best_asos={},\n",
    "            best_aafos={},\n",
    "            best_mean={},\n",
    "        )\n",
    "        \n",
    "        for cutoff in threshold:\n",
    "            cutoff_str = str(cutoff)\n",
    "            results['csi']['asos'][cutoff_str] = []\n",
    "            results['acc']['asos'][cutoff_str] = []\n",
    "            results['best_asos'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "            \n",
    "            results['csi']['aafos'][cutoff_str] = []\n",
    "            results['acc']['aafos'][cutoff_str] = []\n",
    "            results['best_aafos'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "            \n",
    "            results['best_mean'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "        \n",
    "        # Resume 체크\n",
    "        if os.path.exists(f'{output_path}/{seed}/resume.pt'):\n",
    "            resume = torch.load(f'{output_path}/{seed}/resume.pt')\n",
    "            model.load_state_dict(resume['model'])\n",
    "            optimizer.load_state_dict(resume['optimizer'])\n",
    "            scheduler.load_state_dict(resume['scheduler'])\n",
    "            if 'scaler' in resume:\n",
    "                scaler.load_state_dict(resume['scaler'])\n",
    "            start_epoch = resume['epoch'] + 1\n",
    "            results = resume['results']\n",
    "            print(f'Resuming from epoch {start_epoch}...')\n",
    "        \n",
    "        # ====== 학습 루프 ====== #\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "            model.train()\n",
    "            \n",
    "            total_loss_asos = 0.0\n",
    "            total_loss_aafos = 0.0\n",
    "            total_loss = 0.0\n",
    "            \n",
    "            # 데이터셋 길이 동기화\n",
    "            train_dataset.sync_dataset_length()\n",
    "            \n",
    "            pbar = tqdm(train_dataloader, desc=f'Epoch {epoch:2d}')\n",
    "            for batch in pbar:\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # AMP autocast 적용\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    # ASOS 학습\n",
    "                    if ASOS:\n",
    "                        images, label, coords = batch[0]\n",
    "                        loss_asos = train(model, images, label, coords, asos_map_dict, cls_num=0)\n",
    "                    else:\n",
    "                        loss_asos = torch.tensor(0.0).cuda()\n",
    "                    \n",
    "                    # AAFOS 학습\n",
    "                    if AAFOS:\n",
    "                        images, label, coords = batch[1] if ASOS else batch[0]\n",
    "                        loss_aafos = train(model, images, label, coords, aafos_map_dict, cls_num=1)\n",
    "                    else:\n",
    "                        loss_aafos = torch.tensor(0.0).cuda()\n",
    "                    \n",
    "                    loss = asos_weight * loss_asos + aafos_weight * loss_aafos\n",
    "                \n",
    "                # Scaler를 사용한 backward 및 optimizer step\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                \n",
    "                total_loss_asos += loss_asos.item()\n",
    "                total_loss_aafos += loss_aafos.item()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Progress bar 업데이트\n",
    "                pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'asos': f'{loss_asos.item():.4f}'\n",
    "                })\n",
    "            \n",
    "            total_loss_asos /= len(train_dataloader)\n",
    "            total_loss_aafos /= len(train_dataloader)\n",
    "            total_loss /= len(train_dataloader)\n",
    "            \n",
    "            results['loss']['asos'].append(total_loss_asos)\n",
    "            results['loss']['aafos'].append(total_loss_aafos)\n",
    "            results['loss']['total'].append(total_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch:2d} - Total: {total_loss:.4f}, ASOS: {total_loss_asos:.4f}, AAFOS: {total_loss_aafos:.4f}')\n",
    "            scheduler.step()\n",
    "            \n",
    "            # ====== 평가 ====== #\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                if ASOS:\n",
    "                    asos_pred_vec_list = []\n",
    "                    asos_labels_list = []\n",
    "                    \n",
    "                    for batch in test_asos_dataloader:\n",
    "                        images, label, coords = batch[0]\n",
    "                        images = images.cuda()\n",
    "                        \n",
    "                        # 평가 시에도 AMP autocast 적용 (추론 속도 향상)\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            # forward_by_patch는 (B, T*C + misc, H, W) 형태를 받음\n",
    "                            pred_map = model.forward_by_patch(images, patch_size=patch_size)[:, 0]\n",
    "                            pred_map = F.sigmoid(pred_map)\n",
    "                        \n",
    "                        # 관측소 위치에서 prediction 추출\n",
    "                        pred_vec = []\n",
    "                        for x, y in asos_map_dict.values():\n",
    "                            pred_vec.append(pred_map[:, y, x])\n",
    "                        pred_vec = torch.stack(pred_vec, dim=1)\n",
    "                        \n",
    "                        asos_pred_vec_list.append(pred_vec.cpu().numpy())\n",
    "                        asos_labels_list.append(label.numpy())\n",
    "                    \n",
    "                    pred_vecs = np.concatenate(asos_pred_vec_list, axis=0)\n",
    "                    labels = np.concatenate(asos_labels_list, axis=0)\n",
    "                    \n",
    "                    for cutoff in threshold:\n",
    "                        asos_result = calc_measure_valid(labels, pred_vecs, cutoff=cutoff)\n",
    "                        csi, acc, auroc = asos_result[0], asos_result[1], asos_result[2]\n",
    "                        cutoff_str = str(cutoff)\n",
    "                        \n",
    "                        results['csi']['asos'][cutoff_str].append(csi)\n",
    "                        results['acc']['asos'][cutoff_str].append(acc)\n",
    "                        \n",
    "                        is_best = csi > results['best_asos'][cutoff_str]['csi']\n",
    "                        print(f'\\t - ASOS (T={cutoff:.2f}): CSI {csi:.4f}, AUROC {auroc:.4f} {\"*\" if is_best else \"\"}')\n",
    "                        \n",
    "                        if is_best:\n",
    "                            results['best_asos'][cutoff_str]['csi'] = csi\n",
    "                            results['best_asos'][cutoff_str]['epoch'] = epoch\n",
    "                            results['best_asos'][cutoff_str]['model'] = model.state_dict()\n",
    "                        \n",
    "                        # Land/Coast 분리 평가\n",
    "                        asos_land_result = calc_measure_valid(labels[:, :11], pred_vecs[:, :11], cutoff=cutoff)\n",
    "                        asos_coast_result = calc_measure_valid(labels[:, 11:], pred_vecs[:, 11:], cutoff=cutoff)\n",
    "                        print(f'\\t   - Land:  CSI {asos_land_result[0]:.4f}, AUROC {asos_land_result[2]:.4f}')\n",
    "                        print(f'\\t   - Coast: CSI {asos_coast_result[0]:.4f}, AUROC {asos_coast_result[2]:.4f}')\n",
    "                    \n",
    "                    results['auroc']['asos'].append(auroc)\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Resume 저장 (scaler 상태 포함)\n",
    "            resume = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict(),\n",
    "                'scaler': scaler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'results': results\n",
    "            }\n",
    "            os.makedirs(f'{output_path}/{seed}', exist_ok=True)\n",
    "            torch.save(resume, f'{output_path}/{seed}/resume.pt')\n",
    "        \n",
    "        # 최종 저장\n",
    "        torch.save(results, f'{output_path}/{seed}/ckpt.pt')\n",
    "        if os.path.exists(f'{output_path}/{seed}/resume.pt'):\n",
    "            os.remove(f'{output_path}/{seed}/resume.pt')\n",
    "        \n",
    "        print(f'\\nSeed {seed} completed!')\n",
    "        print(f'Best ASOS CSI (T=0.25): {results[\"best_asos\"][\"0.25\"][\"csi\"]:.4f} at epoch {results[\"best_asos\"][\"0.25\"][\"epoch\"]}')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"All seeds completed!\")\n",
    "    print(f\"{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
