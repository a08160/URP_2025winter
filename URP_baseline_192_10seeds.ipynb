{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import netCDF4 as nc\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils import *\n",
    "from model import get_model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from torchvision.ops import sigmoid_focal_loss\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d59b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 실험 설정 ====== # 중요함\n",
    "seeds = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "# 사용할 데이터 종류 설정, 단일 학습인지 멀티테스크인지 설정 (URP 에서 AAFOS FALSE 설정)\n",
    "ASOS = True\n",
    "AAFOS = False\n",
    "assert ASOS or AAFOS, \"At least one of ASOS or AAFOS must be True.\"\n",
    "\n",
    "channels = '16ch' # '16ch' or 'ae304', 16ch일때 [16,512,512]\n",
    "time_range = [-12]\n",
    "resolution = '2km' # '1km':1024 or '2km':512\n",
    "postfix = 'baseline_prediction_192'\n",
    "\n",
    "output_path = \"results/\"\n",
    "output_path += 'asos_' if ASOS else ''\n",
    "output_path += 'aafos_' if AAFOS else ''\n",
    "output_path += channels + '_'\n",
    "output_path += 'time' + str(time_range) + '_'\n",
    "output_path += resolution\n",
    "output_path += ('_' + postfix) if postfix != '' else postfix\n",
    "print(f\"Output path name: {output_path}\")\n",
    "\n",
    "# 기본 asos:aafos 비율 5:1\n",
    "asos_aafos_ratio = 5.0\n",
    "asos_weight = asos_aafos_ratio / (asos_aafos_ratio + 1.0 * AAFOS) if ASOS else 0.0\n",
    "aafos_weight = 1.0 / (asos_aafos_ratio * ASOS + 1.0) if AAFOS else 0.0\n",
    "print(f\"ASOS weight: {asos_weight:.2f}, AAFOS weight: {aafos_weight:.2f}\")\n",
    "\n",
    "# 기타 ablation 설정\n",
    "latlon = False\n",
    "\n",
    "# 실험중인 설정들\n",
    "central_patch = False\n",
    "use_patch = False\n",
    "\n",
    "#  ====== 채널 설정 ====== #\n",
    "if channels == '16ch':\n",
    "    channels_name = ['vi004','vi005','vi006','vi008','nr013','nr016','sw038','wv063','wv069','wv073','ir087','ir096','ir105','ir112','ir123','ir133'] # 시각화 용\n",
    "    channels_calib = ['vi004','vi005','vi006','vi008','nr013','nr016','sw038','wv063','wv069','wv073','ir087','ir096','ir105','ir112','ir123','ir133']\n",
    "    \n",
    "    channels_mean = [1.1912e-01, 1.1464e-01, 1.0734e-01, 1.2504e-01, 5.4983e-02, 9.0381e-02,\n",
    "                2.7813e+02, 2.3720e+02, 2.4464e+02, 2.5130e+02, 2.6948e+02, 2.4890e+02,\n",
    "                2.7121e+02, 2.7071e+02, 2.6886e+02, 2.5737e+02]\n",
    "    channels_std  = [0.1306,  0.1303,  0.1306,  0.1501,  0.0268,  0.0838, 15.8211,  6.1468,\n",
    "                7.8054,  9.3251, 16.4265,  9.6150, 17.2518, 17.6064, 17.0090, 12.5026]\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid channels.\")\n",
    "\n",
    "\n",
    "train_data_info_list = []\n",
    "train_data_info_list.append({\n",
    "    'label_type': 'asos', # 'asos' or 'aafos'\n",
    "    'start_date_str': '20200101', #  라벨기준 일자. KST\n",
    "    'end_date_str': '20230630',\n",
    "    'hour_col_pairs': [(6,'AM')],\n",
    "    'label_keys': ['93','108','112','119','131','133','136','143','146','156','177','102','104','115','138','152','155','159','165','168','169','184','189']\n",
    "}) if ASOS else None\n",
    "\n",
    "test_asos_data_info_list = [\n",
    "    {\n",
    "        'label_type': 'asos', # 'asos' or 'aafos'\n",
    "        'start_date_str': '20230701', #  라벨기준 일자. KST\n",
    "        'end_date_str': '20240630',\n",
    "        'hour_col_pairs': [(6,'AM')],\n",
    "        'label_keys': ['93','108','112','119','131','133','136','143','146','156','177','102','104','115','138','152','155','159','165','168','169','184','189']\n",
    "    },\n",
    "] if ASOS else None\n",
    "\n",
    "\n",
    "origin_size = 900 if resolution == '2km' else 1800\n",
    "image_size = 192  # 384x384 크기로 변경\n",
    "patch_size = 192  # 패치 크기도 384로 변경\n",
    "\n",
    "data_path = '../data/date_kst_URP_384'\n",
    "\n",
    "misc_channels = {\n",
    "    'elevation':'elevation_1km_3600.npy',\n",
    "    'vegetation':'vegetation_1km_3600.npy',\n",
    "    'watermap':'watermap_1km_avg_3600.npy'\n",
    "}\n",
    "lat_lon_path = 'assets/gk2a_ami_ko010lc_latlon.nc'\n",
    "\n",
    "# 학습과 관련된 설정! 변경 시 주의 요함\n",
    "batch_size = 64\n",
    "num_workers = 8\n",
    "\n",
    "epochs = 25\n",
    "lr = 1e-3\n",
    "decay = [10, 20]\n",
    "lr_decay = 0.1\n",
    "weight_decay = 1e-5\n",
    "threshold = [0.25]\n",
    "\n",
    "# 자동으로 설정되는 값\n",
    "asos_x_base, asos_y_base, asos_image_size = get_crop_base(image_size, label_type='asos')\n",
    "aafos_x_base, aafos_y_base, aafos_image_size = get_crop_base(image_size, label_type='aafos')\n",
    "aafos_x_base -= asos_x_base\n",
    "aafos_y_base -= asos_y_base\n",
    "\n",
    "total_channels = len(channels_name) * len(time_range) + len(misc_channels.keys())\n",
    "total_channels += 2 if latlon else 0\n",
    "print(total_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 192의 경우: 384 기준 좌표 계산 후 2로 나눔\n",
    "asos_land_map = {k: coord_to_map(*v, origin_size) for k, v in ASOS_LAND_COORD.items()}\n",
    "asos_land_map = {k: ((v[0]-asos_x_base)//2, (v[1]-asos_y_base)//2) for k, v in asos_land_map.items()}\n",
    "asos_coast_map = {k: coord_to_map(*v, origin_size) for k, v in ASOS_COAST_COORD.items()}\n",
    "asos_coast_map = {k: ((v[0]-asos_x_base)//2, (v[1]-asos_y_base)//2) for k, v in asos_coast_map.items()}\n",
    "asos_map_dict = {**asos_land_map, **asos_coast_map}\n",
    "print(asos_map_dict.keys())\n",
    "\n",
    "CROP_BASE = 64  # 512 -> 384 crop offset\n",
    "CROP_SIZE_384 = 384\n",
    "\n",
    "image = np.load('assets/misc_channels/watermap_1km_avg_3600.npy', allow_pickle=True)\n",
    "image = -image + 1.0\n",
    "image = cv2.resize(image, (512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "image = image[CROP_BASE:CROP_BASE+CROP_SIZE_384, CROP_BASE:CROP_BASE+CROP_SIZE_384]  # 384x384로 crop\n",
    "image = cv2.resize(image, (asos_image_size, asos_image_size), interpolation=cv2.INTER_AREA)  # 384 -> 192로 resize\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image, cmap='gray', vmin=-1, vmax=2)\n",
    "\n",
    "for k, v in asos_land_map.items():\n",
    "    plt.scatter(v[0], v[1], c='r', marker='o', s=10)\n",
    "for k, v in asos_coast_map.items():\n",
    "    plt.scatter(v[0], v[1], c='b', marker='o', s=10)\n",
    "\n",
    "    \n",
    "plt.scatter([], [], c='r', marker='o', label='ASOS Land')\n",
    "plt.scatter([], [], c='b', marker='o', label='ASOS Coast')\n",
    "\n",
    "    \n",
    "plt.title('ASOS Frost Observation Sites')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if channels_mean is None:\n",
    "    date_list = os.listdir(data_path)\n",
    "    date_list = [date for date in date_list if '20200101' <= date < '20240101']\n",
    "\n",
    "    npy_list = []\n",
    "    for date in tqdm(date_list[::10]):\n",
    "        for file in os.listdir(os.path.join(data_path, date)):\n",
    "            if channels not in file:\n",
    "                continue\n",
    "            npy = np.load(os.path.join(data_path, date, file), allow_pickle=True).astype(np.float32) # (C, H, W)\n",
    "            npy_list.append(npy)\n",
    "    npy_array = np.stack(npy_list, axis=0)\n",
    "    print(npy_array.shape) # (N, C, H, W)\n",
    "\n",
    "    channels_mean = npy_array.mean(axis=(0,2,3)).tolist()\n",
    "    channels_std = npy_array.std(axis=(0,2,3)).tolist()\n",
    "\n",
    "channels_mean = channels_mean * len(time_range)\n",
    "channels_std = channels_std * len(time_range)\n",
    "\n",
    "print(channels_mean)\n",
    "print(channels_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c810446",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_data = nc.Dataset(lat_lon_path)\n",
    "lat = lat_lon_data['lat'][:].data\n",
    "lon = lat_lon_data['lon'][:].data\n",
    "\n",
    "lat = cv2.resize(lat, (origin_size, origin_size), interpolation=cv2.INTER_CUBIC)\n",
    "lon = cv2.resize(lon, (origin_size, origin_size), interpolation=cv2.INTER_CUBIC)\n",
    "print(lat.shape, lon.shape)\n",
    "\n",
    "# 192의 경우: 384 영역을 먼저 crop한 후 192로 resize\n",
    "CROP_SIZE_384 = 384\n",
    "asos_lat_384 = lat[asos_y_base:asos_y_base+CROP_SIZE_384, asos_x_base:asos_x_base+CROP_SIZE_384]\n",
    "asos_lon_384 = lon[asos_y_base:asos_y_base+CROP_SIZE_384, asos_x_base:asos_x_base+CROP_SIZE_384]\n",
    "asos_lat = cv2.resize(asos_lat_384.astype(np.float32), (asos_image_size, asos_image_size), interpolation=cv2.INTER_AREA)\n",
    "asos_lon = cv2.resize(asos_lon_384.astype(np.float32), (asos_image_size, asos_image_size), interpolation=cv2.INTER_AREA)\n",
    "print(asos_lat.shape, asos_lon.shape)\n",
    "\n",
    "\n",
    "lcc = ccrs.LambertConformal(central_longitude=126, central_latitude=38, standard_parallels = (30,60))\n",
    "proj = ccrs.PlateCarree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625d22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MISC 0: Elevation (고도)\n",
    "MISC 1: Water Bodies Index (수역 여부)\n",
    "MISC 2: Vegetation Index (식생 여부)\n",
    "\"\"\"\n",
    "CROP_BASE = 64  # 512 -> 384 crop offset\n",
    "CROP_SIZE_384 = 384\n",
    "\n",
    "misc_images = []\n",
    "for misc_channel, misc_path in misc_channels.items():\n",
    "    misc_image = np.load(f'assets/misc_channels/{misc_path}', allow_pickle=True)\n",
    "    # 3600 -> 512로 resize 후, 64:448 crop하여 384x384 생성 (위성 이미지와 동일한 영역)\n",
    "    misc_image = cv2.resize(misc_image, (512, 512), interpolation=cv2.INTER_CUBIC)\n",
    "    misc_image = misc_image[CROP_BASE:CROP_BASE+CROP_SIZE_384, CROP_BASE:CROP_BASE+CROP_SIZE_384]\n",
    "    misc_image = cv2.resize(misc_image, (image_size, image_size), interpolation=cv2.INTER_AREA)  # 384 -> 192로 resize\n",
    "    misc_images.append(misc_image)\n",
    "misc_images = np.stack(misc_images, axis=0)\n",
    "misc_images = torch.tensor(misc_images, dtype=torch.float32)\n",
    "\n",
    "if latlon:\n",
    "    lat_image = cv2.resize(asos_lat, (asos_image_size, asos_image_size), interpolation=cv2.INTER_CUBIC)\n",
    "    lon_image = cv2.resize(asos_lon, (asos_image_size, asos_image_size), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    lat_image = torch.tensor(lat_image, dtype=torch.float32).unsqueeze(0)\n",
    "    lon_image = torch.tensor(lon_image, dtype=torch.float32).unsqueeze(0)\n",
    "    misc_images = torch.cat([misc_images, lat_image, lon_image], dim=0)\n",
    "\n",
    "print(f'MISC images shape: {misc_images.shape}')\n",
    "\n",
    "# normalize by channel\n",
    "misc_mean = misc_images.mean(dim=[1,2], keepdim=True)\n",
    "misc_std = misc_images.std(dim=[1,2], keepdim=True)\n",
    "\n",
    "total_mean = channels_mean + misc_mean.squeeze().tolist()\n",
    "total_std = channels_std + misc_std.squeeze().tolist()\n",
    "\n",
    "print(f'Total mean: {total_mean}')\n",
    "print(f'Total std: {total_std}')\n",
    "\n",
    "fig, axs = plt.subplots(math.ceil(misc_images.shape[0]/3), 3, figsize=(9, 5))\n",
    "for i in range(misc_images.shape[0]):\n",
    "    ax = axs.flatten()[i]\n",
    "    ax.imshow(misc_images[i].numpy(), cmap='viridis')\n",
    "    ax.set_title(f'MISC {i}')\n",
    "    colorbar = plt.colorbar(mappable=ax.images[0], ax=ax, fraction=0.046, pad=0.04)\n",
    "    ax.axis('off')\n",
    "for i in range(misc_images.shape[0], len(axs.flatten())):\n",
    "    ax = axs.flatten()[i]\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "asos_patch_candidate = np.zeros([image_size, image_size], dtype=np.uint8)\n",
    "for x, y in asos_map_dict.values():\n",
    "    y_min = np.clip(y - 3*patch_size//4, 0, image_size - patch_size+1)\n",
    "    y_max = np.clip(y - patch_size//4, 0, image_size - patch_size+1)   \n",
    "    x_min = np.clip(x - 3*patch_size//4, 0, image_size - patch_size+1)\n",
    "    x_max = np.clip(x - patch_size//4, 0, image_size - patch_size+1)\n",
    "    asos_patch_candidate[y_min:y_max, x_min:x_max] = 1\n",
    "    plt.scatter(x, y, c='r', marker='o', s=5)\n",
    "plt.imshow(misc_images[2], cmap='viridis', vmin=-1, vmax=2)\n",
    "plt.imshow(asos_patch_candidate, cmap='gray', vmin=0, vmax=1, alpha=0.5)\n",
    "asos_patch_candidate = np.argwhere(asos_patch_candidate == 1)[:, [1, 0]]\n",
    "print(f'ASOS patch candidates: {asos_patch_candidate}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "patch_candidates = {\n",
    "    'asos': asos_patch_candidate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 192x192 이미지 + ASOS 좌표 시각적 검증 ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 샘플 위성 이미지 로드 (384x384 데이터를 192로 resize)\n",
    "sample_date = '20230101'\n",
    "sample_file = f'{data_path}/{sample_date}/16ch_{sample_date}0600.npy'\n",
    "sample_image = np.load(sample_file).astype(np.float32)\n",
    "\n",
    "# 384 -> 192 resize\n",
    "sample_image_resized = np.zeros((sample_image.shape[0], 192, 192), dtype=np.float32)\n",
    "for i in range(sample_image.shape[0]):\n",
    "    sample_image_resized[i] = cv2.resize(sample_image[i], (192, 192), interpolation=cv2.INTER_AREA)\n",
    "sample_image = sample_image_resized\n",
    "\n",
    "# 2. IR 채널로 배경 표시\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 좌: 위성 이미지 + ASOS 위치\n",
    "ax1 = axes[0]\n",
    "ax1.imshow(sample_image[10], cmap='gray')  # IR087 채널\n",
    "for k, (x, y) in asos_land_map.items():\n",
    "    ax1.scatter(x, y, c='red', s=50, edgecolors='white', linewidth=1)\n",
    "    ax1.annotate(str(k), (x, y), xytext=(3, 3), textcoords='offset points', \n",
    "                fontsize=7, color='red')\n",
    "for k, (x, y) in asos_coast_map.items():\n",
    "    ax1.scatter(x, y, c='blue', s=50, edgecolors='white', linewidth=1)\n",
    "    ax1.annotate(str(k), (x, y), xytext=(3, 3), textcoords='offset points', \n",
    "                fontsize=7, color='blue')\n",
    "ax1.set_title('Satellite Image (IR087) + ASOS Stations')\n",
    "ax1.legend(['Land', 'Coast'], loc='upper right')\n",
    "\n",
    "# 우: 지형 맵(watermap) + ASOS 위치\n",
    "ax2 = axes[1]\n",
    "# misc_images[2]가 watermap\n",
    "watermap = misc_images[2].numpy() if torch.is_tensor(misc_images[2]) else misc_images[2]\n",
    "ax2.imshow(-watermap + 1, cmap='terrain')\n",
    "for k, (x, y) in asos_land_map.items():\n",
    "    ax2.scatter(x, y, c='red', s=50, edgecolors='white', linewidth=1)\n",
    "for k, (x, y) in asos_coast_map.items():\n",
    "    ax2.scatter(x, y, c='blue', s=50, edgecolors='white', linewidth=1)\n",
    "ax2.set_title('Watermap + ASOS Stations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. 좌표 범위 검증 출력\n",
    "print(\"\\n=== ASOS 좌표 범위 검증 ===\")\n",
    "all_coords = list(asos_land_map.values()) + list(asos_coast_map.values())\n",
    "xs = [c[0] for c in all_coords]\n",
    "ys = [c[1] for c in all_coords]\n",
    "print(f\"X 범위: {min(xs)} ~ {max(xs)} (이미지: 0~191)\")\n",
    "print(f\"Y 범위: {min(ys)} ~ {max(ys)} (이미지: 0~191)\")\n",
    "print(f\"범위 내 관측소: {sum(1 for x,y in all_coords if 0<=x<192 and 0<=y<192)}/23\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5302f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=total_mean, std=total_std)\n",
    "])\n",
    "\n",
    "train_dataset = GK2ADataset(data_path=data_path, output_path=output_path, data_info_list=train_data_info_list,\n",
    "                            channels=channels, time_range=time_range, channels_calib=channels_calib, image_size=image_size, misc_images=misc_images,\n",
    "                            patch_size=patch_size, patch_candidates=patch_candidates, transform=transform, train=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "if ASOS:\n",
    "    test_asos_dataset = GK2ADataset(data_path=data_path, output_path=output_path, data_info_list=test_asos_data_info_list,\n",
    "                            channels=channels, time_range=time_range, channels_calib=channels_calib, image_size=image_size, misc_images=misc_images,\n",
    "                            patch_size=patch_size, patch_candidates=None, transform=transform, train=False)\n",
    "    test_asos_dataloader = DataLoader(test_asos_dataset, batch_size=batch_size//2, shuffle=False, num_workers=num_workers, drop_last=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a37e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.patchfy(True)\n",
    "batch = train_dataset[7]\n",
    "for i in range(len(batch)):\n",
    "    images, label, coords = batch[i]\n",
    "    print(images.shape, label.shape, coords.shape)\n",
    "\n",
    "    fig, axs = plt.subplots(math.ceil(images.shape[0]/4), 4, figsize=(12, 2.5 * math.ceil(images.shape[0]/4)), subplot_kw={'projection': lcc})\n",
    "    for i in range(len(axs.flatten())):\n",
    "        ax = axs.flatten()[i]\n",
    "        if i >= images.shape[0]:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        # channel = total_channels[i]\n",
    "        # ax.set_title(channel)\n",
    "\n",
    "        image = np.ones((asos_image_size, asos_image_size), dtype=np.float32)\n",
    "        image *= -1.0\n",
    "        image[coords[1]:coords[1]+patch_size, coords[0]:coords[0]+patch_size] = images[i].numpy()\n",
    "\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        im = ax.pcolormesh(asos_lon, asos_lat, image, cmap='viridis', transform=proj)\n",
    "            \n",
    "        ax.coastlines(resolution='10m', color='black', linewidth=0.5)\n",
    "        ax.add_feature(cfeature.BORDERS.with_scale('10m'), edgecolor='black', linewidth=0.5)\n",
    "\n",
    "        cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705f56ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(total_channels, patch_size, class_num=2, model_type='deeplab')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8965d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_measure_valid(Y_test, Y_test_hat, cutoff=0.5):\n",
    "    Y_test = Y_test.ravel()\n",
    "    Y_test_hat = Y_test_hat.ravel()\n",
    "    \n",
    "    Y_valid = (~np.isnan(Y_test))\n",
    "    Y_test = Y_test[Y_valid]\n",
    "    Y_test_hat = Y_test_hat[Y_valid]\n",
    "\n",
    "    cfmat = confusion_matrix(Y_test, Y_test_hat > cutoff, labels = [0,1])\n",
    "    acc = np.trace(cfmat) / np.sum(cfmat)\n",
    "    csi = cfmat[1,1] /(np.sum(cfmat) - cfmat[0,0] + 1e-8)\n",
    "    \n",
    "    try:\n",
    "        auroc = roc_auc_score(Y_test, Y_test_hat)\n",
    "    except Exception as e:\n",
    "        auroc = 0.0\n",
    "\n",
    "    return csi, acc, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f74cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, images, labels, coords, map_dict, cls_num=0):\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    pred_map = model.forward(images)[:, cls_num] # B, H, W\n",
    "    pred_vec = torch.zeros_like(labels)\n",
    "    for b, (px, py) in enumerate(coords):\n",
    "        for i, (x, y) in enumerate(map_dict.values()):\n",
    "            if px <= x < px + patch_size and py <= y < py + patch_size:\n",
    "                pred_vec[b, i] = pred_map[b, y - py, x - px]\n",
    "            else:\n",
    "                pred_vec[b, i] = labels[b, i]\n",
    "\n",
    "    labels_valid = (~torch.isnan(labels)).float()\n",
    "    labels = torch.nan_to_num(labels, 0.0)\n",
    "\n",
    "    loss_focal_raw = sigmoid_focal_loss(pred_vec, labels, alpha=-1, gamma=2, reduction='none')\n",
    "    loss_focal = (loss_focal_raw * labels_valid).sum() / labels_valid.sum().clamp(min=1.0)\n",
    "    valid_any_batch = (labels_valid.sum() > 0)\n",
    "    all_zero_batch = ((labels * labels_valid).sum() == 0)\n",
    "                \n",
    "    if not (valid_any_batch and all_zero_batch):\n",
    "        loss_non_frost = torch.tensor(0.0, device=labels.device)\n",
    "    else:\n",
    "        loss_non_frost = binary_cross_entropy_with_logits(\n",
    "                        pred_map, torch.zeros_like(pred_map), reduction='mean'\n",
    "                    )\n",
    "\n",
    "    tp = torch.sum(torch.sigmoid(pred_vec) * labels * labels_valid, dim=0)\n",
    "    fn = torch.sum((1-torch.sigmoid(pred_vec)) * labels * labels_valid, dim=0)\n",
    "    fp = torch.sum(torch.sigmoid(pred_vec) * (1-labels) * labels_valid, dim=0)\n",
    "                \n",
    "    loss_csi = torch.mean(-torch.log(tp + 1e-10) + torch.log(tp + fn + fp + 1e-10)) # csi = tp / (tp + fn + fp)\n",
    "\n",
    "    return loss_focal + loss_non_frost + loss_csi\n",
    "    \n",
    "\n",
    "for seed in seeds:\n",
    "    if os.path.exists(f'{output_path}/{seed}/ckpt.pt'):\n",
    "        print(f'Seed {seed} already done. Skipping...')\n",
    "        continue\n",
    "    \n",
    "    model = get_model(total_channels, patch_size)\n",
    "    model.cuda()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=decay, gamma=lr_decay)\n",
    "\n",
    "    start_epoch = 0\n",
    "    \n",
    "    results = dict(\n",
    "            loss = {'asos': [], 'aafos': [], 'total': []},\n",
    "            csi = {'asos': {}, 'aafos': {}},\n",
    "            acc = {'asos': {}, 'aafos': {}},\n",
    "            auroc = {'asos': [], 'aafos': []}, \n",
    "            \n",
    "            best_asos = {},\n",
    "            best_aafos = {},\n",
    "            best_mean = {},\n",
    "        )\n",
    "    \n",
    "    for cutoff_str in threshold:\n",
    "        cutoff_str = str(cutoff_str)\n",
    "        \n",
    "        results['csi']['asos'][cutoff_str] = []\n",
    "        results['acc']['asos'][cutoff_str] = []\n",
    "        results['best_asos'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "        \n",
    "        results['csi']['aafos'][cutoff_str] = []\n",
    "        results['acc']['aafos'][cutoff_str] = []\n",
    "        results['best_aafos'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "        \n",
    "        results['best_mean'][cutoff_str] = {'csi': 0.0, 'epoch': -1, 'model': None}\n",
    "    \n",
    "    if os.path.exists(f'{output_path}/{seed}/resume.pt'):\n",
    "        resume = torch.load(f'{output_path}/{seed}/resume.pt')\n",
    "        model.load_state_dict(resume['model'])\n",
    "        optimizer.load_state_dict(resume['optimizer'])\n",
    "        scheduler.load_state_dict(resume['scheduler'])\n",
    "        start_epoch = resume['epoch'] + 1\n",
    "        results = resume['results']\n",
    "        print(f'Resuming from epoch {start_epoch}...')\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        \n",
    "        total_loss_asos = 0.0\n",
    "        total_loss_aafos = 0.0\n",
    "        total_loss = 0.0\n",
    "                \n",
    "        # asos와 aafos 데이터셋 간 길이 맞춤 / 매 epoch마다 실행\n",
    "        train_dataset.sync_dataset_length()\n",
    "        for batch in tqdm(train_dataloader):\n",
    "            if ASOS:\n",
    "                images, label, coords = batch[0]\n",
    "                loss_asos = train(model, images, label, coords, asos_map_dict, cls_num=0)\n",
    "            else:\n",
    "                loss_asos = torch.tensor(0.0).cuda()\n",
    "            \n",
    "            if AAFOS:\n",
    "                images, label, coords = batch[1] if ASOS else batch[0]\n",
    "                loss_aafos = train(model, images, label, coords, aafos_map_dict, cls_num=1)\n",
    "            else:\n",
    "                loss_aafos = torch.tensor(0.0).cuda()\n",
    "\n",
    "            loss = asos_weight * loss_asos + aafos_weight * loss_aafos\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss_asos += loss_asos.item()\n",
    "            total_loss_aafos += loss_aafos.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        total_loss_asos = total_loss_asos / len(train_dataloader)\n",
    "        total_loss_aafos = total_loss_aafos / len(train_dataloader)\n",
    "        total_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        results['loss']['asos'].append(total_loss_asos)\n",
    "        results['loss']['aafos'].append(total_loss_aafos)\n",
    "        results['loss']['total'].append(total_loss)\n",
    "\n",
    "        print(f'Epoch {epoch:2d} - Total Loss: {total_loss:.4f}, ASOS Loss: {total_loss_asos:.4f}, AAFOS Loss: {total_loss_aafos:.4f}')\n",
    "        # epoch loop에 있으니까 -> 10, 20 epoch마다 lr decay\n",
    "        scheduler.step()\n",
    "    \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            asos_results_per_threshold = {}\n",
    "            \n",
    "            if ASOS:\n",
    "                asos_pred_vec_list = []\n",
    "                asos_labels_list = []\n",
    "                for batch in test_asos_dataloader:\n",
    "                    images, label, coords = batch[0]\n",
    "                    images = images.cuda()\n",
    "                    \n",
    "                    pred_map = model.forward_by_patch(images)[:,0]\n",
    "                    pred_map = F.sigmoid(pred_map)\n",
    "                    \n",
    "                    pred_vec = []\n",
    "                    for x, y in asos_map_dict.values():\n",
    "                        pred_vec.append(pred_map[:, y, x])\n",
    "                    pred_vec = torch.stack(pred_vec, dim=1)\n",
    "                    \n",
    "                    asos_pred_vec_list.append(pred_vec.cpu().numpy())\n",
    "                    asos_labels_list.append(label.numpy())\n",
    "                    \n",
    "                pred_vecs = np.concatenate(asos_pred_vec_list, axis=0)\n",
    "                labels = np.concatenate(asos_labels_list, axis=0)\n",
    "                \n",
    "                for cutoff in threshold:\n",
    "                    asos_result = calc_measure_valid(labels, pred_vecs, cutoff=cutoff)\n",
    "                    csi, acc, auroc = asos_result[0], asos_result[1], asos_result[2]\n",
    "\n",
    "                    cutoff_str = str(cutoff)\n",
    "                    \n",
    "                    results['csi']['asos'][cutoff_str].append(csi)\n",
    "                    results['acc']['asos'][cutoff_str].append(acc)\n",
    "                    \n",
    "                    asos_results_per_threshold[cutoff_str] = asos_result\n",
    "                    \n",
    "                    is_best = csi > results['best_asos'][cutoff_str]['csi']\n",
    "                    print(f'\\t - ASOS  (T={cutoff:.2f}): CSI {csi:.4f}, AUROC {auroc:.4f} {\"*\" if is_best else \"\"}')\n",
    "                    \n",
    "                    if is_best:\n",
    "                        results['best_asos'][cutoff_str]['csi'] = csi\n",
    "                        results['best_asos'][cutoff_str]['epoch'] = epoch\n",
    "                        results['best_asos'][cutoff_str]['model'] = model.state_dict()\n",
    "\n",
    "                    asos_land_result = calc_measure_valid(labels[:,:11], pred_vecs[:,:11], cutoff=cutoff)\n",
    "                    asos_coast_result = calc_measure_valid(labels[:,11:], pred_vecs[:,11:], cutoff=cutoff)\n",
    "                    print(f'\\t   - ASOS Land: CSI {asos_land_result[0]:.4f}, AUROC {asos_land_result[2]:.4f}')\n",
    "                    print(f'\\t   - ASOS Coast: CSI {asos_coast_result[0]:.4f}, AUROC {asos_coast_result[2]:.4f}')\n",
    "\n",
    "                    log_dir = f'{output_path}/{seed}'\n",
    "                    log_path = f'{log_dir}/train_log.csv'\n",
    "                    \n",
    "                    # 2. 폴더가 없으면 먼저 생성 (이 부분이 추가됨)\n",
    "                    os.makedirs(log_dir, exist_ok=True)\n",
    "                    \n",
    "                    # 3. 파일 기록\n",
    "                    is_first_write = not os.path.exists(log_path)\n",
    "                    \n",
    "                    with open(log_path, 'a') as log_f:\n",
    "                        if is_first_write:\n",
    "                            log_f.write('Epoch,Threshold,ASOS_Loss,CSI,AUROC,Land_CSI,Coast_CSI\\n')\n",
    "                        \n",
    "                        log_line = (f'{epoch},{cutoff},{total_loss_asos:.6f},{csi:.6f},{auroc:.6f},'\n",
    "                                    f'{asos_land_result[0]:.6f},{asos_coast_result[0]:.6f}\\n')\n",
    "                        log_f.write(log_line)\n",
    "\n",
    "                results['auroc']['asos'].append(auroc)\n",
    "                            \n",
    "            print()\n",
    "        \n",
    "        resume = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'results': results\n",
    "        }\n",
    "        os.makedirs(f'{output_path}/{seed}', exist_ok=True)\n",
    "        torch.save(resume, f'{output_path}/{seed}/resume.pt')\n",
    "    \n",
    "    torch.save(results, f'{output_path}/{seed}/ckpt.pt')\n",
    "    os.remove(f'{output_path}/{seed}/resume.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ab295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if ASOS and not AAFOS:\n",
    "    mode = 'best_asos'\n",
    "\n",
    "results_df = pd.DataFrame(columns=['type','cutoff','label', 'csi', 'acc', 'auroc'])\n",
    "\n",
    "results_dict = {}\n",
    "for cutoff in threshold:\n",
    "    results_dict[str(cutoff)] = {\n",
    "        'asos': {},\n",
    "        'aafos': {}\n",
    "    }\n",
    "\n",
    "for seed in seeds:\n",
    "    ckpt_path = f'{output_path}/{seed}/ckpt.pt'\n",
    "    if not os.path.exists(ckpt_path):\n",
    "        print(f'Seed {seed} not found. Skipping...')\n",
    "        continue\n",
    "    \n",
    "    ckpt = torch.load(ckpt_path)\n",
    "    print(f'--- Processing Seed {seed} ({ckpt_path}) ---')\n",
    "\n",
    "    model = get_model(total_channels, patch_size)\n",
    "    model.cuda()\n",
    "    \n",
    "    for cutoff in threshold:\n",
    "        cutoff_str = str(cutoff)\n",
    "        \n",
    "        if cutoff_str not in ckpt[mode]:\n",
    "            print(f\"  Warning: T={cutoff_str} not found for mode '{mode}' in seed {seed}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            model_data = ckpt[mode][cutoff_str]\n",
    "            model.load_state_dict(model_data['model'])\n",
    "            model.eval()\n",
    "            print(f\"  T={cutoff_str}: Loaded epoch {model_data['epoch']} with {mode} CSI {model_data['csi']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error loading model for T={cutoff_str}, Seed={seed}: {e}. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if ASOS:\n",
    "                asos_pred_vec_list = []\n",
    "                asos_labels_list = []\n",
    "                \n",
    "                for batch in tqdm(test_asos_dataloader, desc=f\"ASOS Eval (Seed {seed}, T={cutoff_str})\"):\n",
    "                    images, label, coords = batch[0]\n",
    "                    images = images.cuda()\n",
    "                    \n",
    "                    pred_map = model.forward_by_patch(images)[:,0]\n",
    "                    pred_map = F.sigmoid(pred_map)\n",
    "                    \n",
    "                    pred_vec = []\n",
    "                    for x, y in asos_map_dict.values():\n",
    "                        pred_vec.append(pred_map[:, y, x])\n",
    "                    pred_vec = torch.stack(pred_vec, dim=1)\n",
    "                    \n",
    "                    asos_pred_vec_list.append(pred_vec.cpu().numpy())\n",
    "                    asos_labels_list.append(label.numpy())\n",
    "                        \n",
    "                pred_vecs = np.concatenate(asos_pred_vec_list, axis=0)\n",
    "                labels = np.concatenate(asos_labels_list, axis=0)\n",
    "                \n",
    "                label_cols = [col for _, col in test_asos_data_info_list[0]['hour_col_pairs']]\n",
    "                for col in label_cols:\n",
    "                    indices = [i for i, data_dict in enumerate(test_asos_dataset.data_info_list[0]['data_dict_list']) if data_dict['label_col'] == col]\n",
    "                    pred_vec_selected = pred_vecs[indices]\n",
    "                    labels_selected = labels[indices]\n",
    "                    \n",
    "                    results = calc_measure_valid(labels_selected, pred_vec_selected, cutoff=cutoff)\n",
    "                    \n",
    "                    results_dict[cutoff_str]['asos'].setdefault(col, {})\n",
    "                    results_dict[cutoff_str]['asos'][col].setdefault('csi', []).append(results[0])\n",
    "                    results_dict[cutoff_str]['asos'][col].setdefault('acc', []).append(results[1])\n",
    "                    results_dict[cutoff_str]['asos'][col].setdefault('auroc', []).append(results[2])\n",
    "\n",
    "\n",
    "all_results_rows = []\n",
    "\n",
    "for cutoff_str, type_dict_for_thr in results_dict.items():\n",
    "    for data_type, type_dict in type_dict_for_thr.items():\n",
    "        csi_mean_list = []\n",
    "        acc_mean_list = []\n",
    "        auroc_mean_list = []\n",
    "        \n",
    "        for label_col, metrics_dict in type_dict.items():\n",
    "            \n",
    "            if not metrics_dict.get('csi'):\n",
    "                print(f\"Warning: No data found for T={cutoff_str}, Type={data_type}, Label={label_col}. Skipping row.\")\n",
    "                continue\n",
    "\n",
    "            csi_mean = np.mean(metrics_dict['csi'])\n",
    "            csi_mean_list.append(csi_mean)\n",
    "            csi_std = np.std(metrics_dict['csi'])\n",
    "            \n",
    "            acc_mean = np.mean(metrics_dict['acc']) \n",
    "            acc_mean_list.append(acc_mean)\n",
    "            acc_std = np.std(metrics_dict['acc'])\n",
    "            \n",
    "            auroc_mean = np.mean(metrics_dict['auroc'])\n",
    "            auroc_mean_list.append(auroc_mean)\n",
    "            auroc_std = np.std(metrics_dict['auroc'])\n",
    "\n",
    "            all_results_rows.append({\n",
    "                'type': data_type,\n",
    "                'threshold': cutoff_str,  \n",
    "                'label': label_col,\n",
    "                'csi': f'{csi_mean:.4f}',\n",
    "                'acc': f'{acc_mean:.4f}',\n",
    "                'auroc': f'{auroc_mean:.4f}',\n",
    "            })\n",
    "            \n",
    "        all_results_rows.append({\n",
    "            'type': data_type,\n",
    "            'threshold': cutoff_str,\n",
    "            'label': 'mean',\n",
    "            'csi': f'{np.mean(csi_mean_list):.4f}',\n",
    "            'acc': f'{np.mean(acc_mean_list):.4f}',\n",
    "            'auroc': f'{np.mean(auroc_mean_list):.4f}',\n",
    "        })\n",
    "\n",
    "\n",
    "if all_results_rows:\n",
    "    results_df = pd.DataFrame(all_results_rows)\n",
    "else:\n",
    "    print(\"Warning: No results were generated.\")\n",
    "print(results_df) \n",
    "\n",
    "output_csv_path = f'{output_path}/final_results_{mode}.csv'\n",
    "results_df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nFinal results saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58419d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "water_map = np.load('./assets/misc_channels/watermap_1km_avg_1800.npy')\n",
    "water_map = cv2.resize(water_map, (image_size, image_size), interpolation=cv2.INTER_CUBIC)\n",
    "water_mask = -1.0 * water_map + 1.0\n",
    "print(water_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6f1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "frost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
